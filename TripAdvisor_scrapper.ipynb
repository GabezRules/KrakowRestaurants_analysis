{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROME_PATH = ''\n",
    "PATH_TO_SAVE_COMMENTS = ''\n",
    "PATH_TO_SAVE_LINKS = ''\n",
    "PATH_TO_SAVE_RESTAURANTS = ''\n",
    "\n",
    "MAX_LINKS = 10\n",
    "\n",
    "df = pd.DataFrame({\"quote\":[], \"content\":[],\"rating\":[],\"date\":[],\"place\":[]})\n",
    "links_list = \"\"\n",
    "all_data = []\n",
    "restaurant_data = []\n",
    "place_info = \"\"\n",
    "list_seen = []\n",
    "\n",
    "main_url = 'https://pl.tripadvisor.com/Restaurants-g274772-Krakow_Lesser_Poland_Province_Southern_Poland.html'\n",
    "url_list = []\n",
    "\n",
    "#collector = LinkCollector(main_url)\n",
    "#collector.setup_link_collector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_scrapper_and_start_process():\n",
    "    scrapper = SinglePageScrapper()\n",
    "    links_list = pd.read_csv(PATH_TO_SAVE_LINKS)['url'].tolist()\n",
    "    links_list = list(reversed(links_list))\n",
    "    \n",
    "    while len(links_list) > 0 :\n",
    "        scrapper.setup_work(links_list[0])\n",
    "        links_list = links_list[1:]\n",
    "        \n",
    "    scrapper.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    all_data.append(row)\n",
    "    \n",
    "def save_dataframe():\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(PATH_TO_SAVE_COMMENTS, index = False, header=True)\n",
    "    \n",
    "def save_restaurant_info(row):\n",
    "    restaurant_data.append(row)\n",
    "    \n",
    "def save_places():\n",
    "    df = pd.DataFrame(restaurant_data)\n",
    "    df.to_csv(+PATH_TO_SAVE_RESTAURANTS, index = False, header=True)\n",
    "    \n",
    "def save_links():\n",
    "    df = pd.DataFrame(url_list)\n",
    "    df.to_csv(PATH_TO_SAVE_LINKS, index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePageScrapper:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome(CHROME_PATH)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.height = \"\"\n",
    "        self.url = \"\"\n",
    "        self.soup = \"\"\n",
    "        self.reviev_links = []\n",
    "        self.place_name = \"\"\n",
    "        \n",
    "    def end_work(self):\n",
    "        save_dataframe()\n",
    "        save_places()\n",
    "        \n",
    "    def setup_work(self,url):\n",
    "        self.url = url\n",
    "        self.driver.get(self.url)\n",
    "        source = self.driver.page_source\n",
    "        self.soup = BeautifulSoup(source, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            m_place_name = self.soup.find('h1',{'class':'_3a1XQ88S'}).getText()\n",
    "            self.place_name = m_place_name\n",
    "        except: print(\"error\")\n",
    "        \n",
    "        self.scroll_down()\n",
    "        self.get_review_links()\n",
    "        self.start_scrapping()\n",
    "        \n",
    "    def get_place_info(self):\n",
    "        try:\n",
    "            m_place_name = self.soup.find('h1',{'class':'_3a1XQ88S'}).getText()\n",
    "            self.place_name = m_place_name\n",
    "        except: print(\"error\")\n",
    "        \n",
    "        place_info = self.soup.find('span',{'class':'_13OzAOXO _34GKdBMV'})\n",
    "        m_prices_info = len(place_info.find_all('a')[0].getText())\n",
    "        \n",
    "        detail_info = self.soup.find('div',{'class':'_3UjHBXYa'})\n",
    "        detail_info = detail_info.find_all('div',{'class':'_1XLfiSsv'})\n",
    "        \n",
    "        m_cuisine_type = detail_info[1].getText()\n",
    "        m_others = detail_info[2].getText()\n",
    "            \n",
    "        num_rec = self.soup.find('div',{'class':'Ct2OcWS4 _3nlSp84z'})\n",
    "        \n",
    "        m_num_rec = num_rec.find('a',{\"class\":'_10Iv7dOs'}).getText()\n",
    "        \n",
    "        m_avg_rec = self.soup.find('span',{'class':'r2Cf69qf'}).getText()\n",
    "        \n",
    "        m_place = self.soup.find('span',{'class':'_2saB_OSe _1OBMr94N'}).find_all('div')[0].getText()\n",
    "        \n",
    "        m_obj = {'name':self.place_name,'prices':m_prices_info,'cuisine':m_cuisine_type,'others':m_others,'num_rec':m_num_rec,'avg_rec':m_avg_rec,'location':m_place}\n",
    "        \n",
    "        save_restaurant_info(m_obj)\n",
    "        \n",
    "    def scroll_down(self):\n",
    "        speed = 20\n",
    "        current_scroll_position = 0\n",
    "        new_height = 1\n",
    "        while current_scroll_position <= new_height:\n",
    "            current_scroll_position += speed\n",
    "            self.driver.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "    def get_review_links(self):\n",
    "        comment_elements = self.soup.find(\"div\",{\"class\":\"pageNumbers\"}).find_all('a')\n",
    "        for ele in comment_elements:\n",
    "            if ele.has_attr('href'):\n",
    "                a_url = \"https://pl.tripadvisor.com\"+ele.attrs['href']\n",
    "                self.reviev_links.append(a_url)\n",
    "            \n",
    "    def start_scrapping(self):\n",
    "        self.get_place_info()\n",
    "        \n",
    "        for ele in self.reviev_links:\n",
    "            self.load_info(ele)\n",
    "            \n",
    "        self.end_work()\n",
    "            \n",
    "    def get_rating(self,m_str): \n",
    "        m_str = ''.join(reversed(m_str))\n",
    "        return m_str[1]\n",
    "            \n",
    "    def load_info(self, link):\n",
    "        self.driver.get(link)\n",
    "        source = self.driver.page_source\n",
    "        self.soup = BeautifulSoup(source, 'html.parser')\n",
    "        \n",
    "        r_divs = self.soup.find_all(\"div\",{\"class\":\"review-container\"})\n",
    "        \n",
    "        r_selenium_divs = self.driver.find_elements_by_class_name('review-container')          \n",
    "        \n",
    "        for ele in r_divs:\n",
    "            m_index = r_divs.index(ele)\n",
    "            \n",
    "            try:\n",
    "                click_p = r_selenium_divs[m_index].find_element_by_xpath(\"//p[contains(@class,'partial_entry')]\")\n",
    "                click_span = click_p.find_element_by_xpath(\"//span[contains(@class,'taLnk ulBlueLinks')]\") \n",
    "                ActionChains(self.driver).move_to_element(click_span).click().perform()\n",
    "            except:\n",
    "                print(\"No 'more' span!\") \n",
    "            \n",
    "            m_quote = ele.find(\"div\",{\"class\":\"quote\"}).getText()\n",
    "            m_content = ele.find(\"p\",{\"class\":\"partial_entry\"}).getText()\n",
    "            m_stay_date = \"\"\n",
    "\n",
    "            try:\n",
    "                stay_date_div = ele.find(\"div\",{\"class\":\"prw_rup prw_reviews_stay_date_hsx\"})\n",
    "                stay_date_list = stay_date_div.getText().split(\" \")\n",
    "                stay_date_list = list(reversed(stay_date_list))\n",
    "                m_stay_date = stay_date_list[1]+' '+stay_date_list[0]\n",
    "            except: print('error')\n",
    "\n",
    "            rating_container = self.soup.find(\"div\",{\"class\":\"ui_column is-9\"}).find_all(\"span\")[0]\n",
    "            m_rating = self.get_rating(rating_container['class'][1]) \n",
    "            \n",
    "            ele_html = ele.encode_contents()\n",
    "            \n",
    "            temp_list = {\"quote\":m_quote, \"content\":m_content,\"rating\":m_rating,\"date\":m_stay_date, \"place\":self.place_name}\n",
    "            print(temp_list)\n",
    "            process_row(temp_list)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkCollector:\n",
    "    def __init__(self,url):\n",
    "        self.driver = webdriver.Chrome(CHROME_PATH)\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.height = \"\"\n",
    "        self.url = url\n",
    "        self.soup = \"\"\n",
    "        self.reviev_links = []\n",
    "        self.place_name = []\n",
    "        global url_list\n",
    "              \n",
    "    def setup_link_collector(self):\n",
    "        self.scroll_down()\n",
    "        self.driver.get(self.url)\n",
    "        source = self.driver.page_source\n",
    "        self.soup = BeautifulSoup(source,'html.parser')\n",
    "        repeated = False\n",
    "        next_span_disabled='a'\n",
    "        \n",
    "        try:\n",
    "            next_span_disabled = self.driver.find_element_by_xpath(\"//span[contains(@class,'nav next disabled')]\")\n",
    "        except: print(\"not yet!\")\n",
    "        \n",
    "        while next_span_disabled == 'a' or len(url_list) < MAX_LINKS :\n",
    "            self.scroll_down()\n",
    "            link_list = self.soup.find_all('div',{'class':'_1kNOY9zw'})\n",
    "            \n",
    "            try:\n",
    "                next_btn = self.driver.find_element_by_xpath(\"//a[contains(@class,'nav next rndBtn ui_button primary taLnk')]\")\n",
    "            except: print(\"end of scrapping!\")\n",
    "                \n",
    "            for ele in link_list:\n",
    "                n_link = \"https://pl.tripadvisor.com\"+ele.find('a',{'class':'_15_ydu6b'}).attrs['href']\n",
    "                for i in url_list: \n",
    "                    if i['url'] == n_link: \n",
    "                        repeated = True\n",
    "                    \n",
    "                if not repeated: url_list.append({\"url\":n_link, \"visited\": False})\n",
    "            \n",
    "            try:\n",
    "                ActionChains(self.driver).move_to_element(next_btn).click().perform()\n",
    "            except: print(\"end of scrapping!\")\n",
    "                    \n",
    "            try:\n",
    "                next_span_disabled = self.driver.find_element_by_xpath(\"//span[contains(@class,'nav next disabled')]\")\n",
    "            except: print(\"not yet!\")\n",
    "            \n",
    "        save_links()\n",
    "        setup_scrapper_and_start_process()\n",
    "        \n",
    "    def scroll_down(self):\n",
    "        speed = 40\n",
    "        current_scroll_position = 0\n",
    "        new_height = 1\n",
    "        while current_scroll_position <= new_height:\n",
    "            current_scroll_position += speed\n",
    "            self.driver.execute_script(\"window.scrollTo(0, {});\".format(current_scroll_position))\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
